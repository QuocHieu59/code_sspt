{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":145.504653,"end_time":"2025-06-20T20:58:25.080336","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-20T20:55:59.575683","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a4f261ae","cell_type":"code","source":"!git clone https://github.com/QuocHieu59/code_sspt.git","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-06-21T13:07:46.234557Z","iopub.execute_input":"2025-06-21T13:07:46.234861Z","iopub.status.idle":"2025-06-21T13:07:47.632607Z","shell.execute_reply.started":"2025-06-21T13:07:46.234838Z","shell.execute_reply":"2025-06-21T13:07:47.631830Z"},"papermill":{"duration":1.210483,"end_time":"2025-06-20T20:56:07.034293","exception":false,"start_time":"2025-06-20T20:56:05.823810","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'code_sspt'...\nremote: Enumerating objects: 12, done.\u001b[K\nremote: Counting objects: 100% (12/12), done.\u001b[K\nremote: Compressing objects: 100% (8/8), done.\u001b[K\nremote: Total 12 (delta 2), reused 12 (delta 2), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (12/12), 31.17 KiB | 1.20 MiB/s, done.\nResolving deltas: 100% (2/2), done.\n","output_type":"stream"}],"execution_count":2},{"id":"b7eaacf8","cell_type":"code","source":"%cd /kaggle/working/code_sspt/Qwen3-0.6b-2GPU","metadata":{"execution":{"iopub.status.busy":"2025-06-21T13:07:47.634117Z","iopub.execute_input":"2025-06-21T13:07:47.634440Z","iopub.status.idle":"2025-06-21T13:07:47.645616Z","shell.execute_reply.started":"2025-06-21T13:07:47.634405Z","shell.execute_reply":"2025-06-21T13:07:47.644924Z"},"papermill":{"duration":0.009653,"end_time":"2025-06-20T20:56:07.045845","exception":false,"start_time":"2025-06-20T20:56:07.036192","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/code_sspt/Qwen3-0.6b-2GPU\n","output_type":"stream"}],"execution_count":3},{"id":"51992bd7","cell_type":"code","source":"!git pull","metadata":{"execution":{"iopub.status.busy":"2025-06-21T13:07:47.646302Z","iopub.execute_input":"2025-06-21T13:07:47.646518Z","iopub.status.idle":"2025-06-21T13:07:48.006994Z","shell.execute_reply.started":"2025-06-21T13:07:47.646503Z","shell.execute_reply":"2025-06-21T13:07:48.006331Z"},"papermill":{"duration":0.319127,"end_time":"2025-06-20T20:56:07.366465","exception":false,"start_time":"2025-06-20T20:56:07.047338","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Already up to date.\n","output_type":"stream"}],"execution_count":4},{"id":"30f6f5bb","cell_type":"code","source":"!torchrun --nproc_per_node=2 PARADIS-Qwen3-FSDP.py","metadata":{"execution":{"iopub.status.busy":"2025-06-21T13:07:48.008785Z","iopub.execute_input":"2025-06-21T13:07:48.009419Z","iopub.status.idle":"2025-06-21T15:50:41.679048Z","shell.execute_reply.started":"2025-06-21T13:07:48.009391Z","shell.execute_reply":"2025-06-21T15:50:41.678322Z"},"papermill":{"duration":137.38102,"end_time":"2025-06-20T20:58:24.749150","exception":false,"start_time":"2025-06-20T20:56:07.368130","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"W0621 13:07:57.962000 96 torch/distributed/run.py:792] \nW0621 13:07:57.962000 96 torch/distributed/run.py:792] *****************************************\nW0621 13:07:57.962000 96 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0621 13:07:57.962000 96 torch/distributed/run.py:792] *****************************************\n2025-06-21 13:08:17.569106: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-21 13:08:17.569107: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750511298.055977      99 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750511298.055953      98 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750511298.178865      99 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1750511298.178880      98 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[Rank 0] Starting with world_size = 2\n[Rank 1] Starting with world_size = 2\n[Rank 0] Process group initialized!\n[Rank 0] Backend: nccl\n[Rank 0] World Size: 2\n[Rank 0] NCCL_DEBUG: NOT SET\n[Rank 1] Process group initialized!\n[Rank 1] Backend: nccl\n[Rank 1] World Size: 2\n[Rank 1] NCCL_DEBUG: NOT SET\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtungnguyen19995969\u001b[0m (\u001b[33mtungnguyen19995969-hanoi-university-of-science-and-techn\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtungnguyen19995969\u001b[0m (\u001b[33mtungnguyen19995969-hanoi-university-of-science-and-techn\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `shared` mode feature is experimental and may change. Please contact support@wandb.com for guidance and to report any issues.\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/code_sspt/Qwen3-0.6b-2GPU/wandb/run-20250621_130840-br6n0x73\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFSDP\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tungnguyen19995969-hanoi-university-of-science-and-techn/PARADIS-Qwen3_0.6B\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tungnguyen19995969-hanoi-university-of-science-and-techn/PARADIS-Qwen3_0.6B/runs/br6n0x73\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `shared` mode feature is experimental and may change. Please contact support@wandb.com for guidance and to report any issues.\n\n==================================================\nGeneral setup has been done!\n==================================================\n\n==================================================\nModel & Tokenizer\n==================================================\nLoading tokenizer and model...\ntokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.73k/9.73k [00:00<00:00, 14.9MB/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/code_sspt/Qwen3-0.6b-2GPU/wandb/run-20250621_130843-br6n0x73\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFSDP\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tungnguyen19995969-hanoi-university-of-science-and-techn/PARADIS-Qwen3_0.6B\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tungnguyen19995969-hanoi-university-of-science-and-techn/PARADIS-Qwen3_0.6B/runs/br6n0x73\u001b[0m\nvocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.78M/2.78M [00:00<00:00, 30.2MB/s]\nmerges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.67M/1.67M [00:00<00:00, 17.0MB/s]\ntokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11.4M/11.4M [00:00<00:00, 17.5MB/s]\nconfig.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 726/726 [00:00<00:00, 2.00MB/s]\nmodel.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.50G/1.50G [00:06<00:00, 221MB/s]\ngeneration_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 239/239 [00:00<00:00, 503kB/s]\nModel loaded. Parameters: 596,049,920\n\n==================================================\nDataset\n==================================================\nREADME.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 632/632 [00:00<00:00, 1.42MB/s]\n(‚Ä¶)-00000-of-00003-6218d2963e302058.parquet: 100%|‚ñà| 245M/245M [00:01<00:00, 219\n(‚Ä¶)-00001-of-00003-12e6c4fadbec91d4.parquet: 100%|‚ñà| 55.2M/55.2M [00:00<00:00, 2\n(‚Ä¶)-00002-of-00003-175fcfe1c45b0b85.parquet: 100%|‚ñà| 270M/270M [00:01<00:00, 263\nGenerating train split: 100%|‚ñà| 1284930/1284930 [00:04<00:00, 311736.45 examples\nFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1284930/1284930 [00:09<00:00, 131832.32 examples/s]\nFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1284930/1284930 [00:09<00:00, 131732.82 examples/s]\nTotal: 1263196 samples\nTrain samples: 10000\nValid samples: 10000\nTrain batches: 2500\nValid batches: 2500\n\n==================================================\nOptimizer & scheduler\n==================================================\nTotal training steps: 937\nWarmup steps: 93\n\n==================================================\nEpoch 1/3\n==================================================\nTraining...\n[Rank 1] Step 40/2500, Loss: 2.9741, LR: 2.69e-06[Rank 0] Step 40/2500, Loss: 2.9288, LR: 2.69e-06\n\n[Rank 0] Step 80/2500, Loss: 2.6711, LR: 5.38e-06\n[Rank 1] Step 80/2500, Loss: 2.6834, LR: 5.38e-06\n[Rank 0] Step 120/2500, Loss: 2.5082, LR: 8.06e-06[Rank 1] Step 120/2500, Loss: 2.4578, LR: 8.06e-06\n\n[Rank 0] Step 160/2500, Loss: 2.3789, LR: 1.08e-05\n[Rank 1] Step 160/2500, Loss: 2.3369, LR: 1.08e-05\n[Rank 0] Step 200/2500, Loss: 2.2520, LR: 1.34e-05\n[Rank 1] Step 200/2500, Loss: 2.2570, LR: 1.34e-05\n[Rank 0] Step 240/2500, Loss: 2.1596, LR: 1.61e-05[Rank 1] Step 240/2500, Loss: 2.1753, LR: 1.61e-05\n\n[Rank 1] Step 280/2500, Loss: 2.0951, LR: 1.88e-05[Rank 0] Step 280/2500, Loss: 2.0864, LR: 1.88e-05\n\n[Rank 1] Step 320/2500, Loss: 2.0380, LR: 2.15e-05[Rank 0] Step 320/2500, Loss: 2.0018, LR: 2.15e-05\n\n[Rank 0] Step 360/2500, Loss: 1.9519, LR: 2.42e-05[Rank 1] Step 360/2500, Loss: 2.0083, LR: 2.42e-05\n\n[Rank 0] Step 400/2500, Loss: 1.9316, LR: 2.69e-05\n[Rank 1] Step 400/2500, Loss: 1.9623, LR: 2.69e-05\n[Rank 1] Step 440/2500, Loss: 1.9379, LR: 2.96e-05[Rank 0] Step 440/2500, Loss: 1.8888, LR: 2.96e-05\n\n[Rank 1] Step 480/2500, Loss: 1.9074, LR: 3.23e-05[Rank 0] Step 480/2500, Loss: 1.8657, LR: 3.23e-05\n\n[Rank 1] Step 520/2500, Loss: 1.8865, LR: 3.49e-05[Rank 0] Step 520/2500, Loss: 1.8360, LR: 3.49e-05\n\n[Rank 1] Step 560/2500, Loss: 1.8623, LR: 3.76e-05[Rank 0] Step 560/2500, Loss: 1.8143, LR: 3.76e-05\n\n[Rank 0] Step 600/2500, Loss: 1.7878, LR: 4.03e-05[Rank 1] Step 600/2500, Loss: 1.8550, LR: 4.03e-05\n\n[Rank 0] Step 640/2500, Loss: 1.7764, LR: 4.30e-05\n[Rank 1] Step 640/2500, Loss: 1.8321, LR: 4.30e-05\n[Rank 0] Step 680/2500, Loss: 1.7609, LR: 4.57e-05\n[Rank 1] Step 680/2500, Loss: 1.8182, LR: 4.57e-05\n[Rank 0] Step 720/2500, Loss: 1.7535, LR: 4.84e-05\n[Rank 1] Step 720/2500, Loss: 1.8048, LR: 4.84e-05\n[Rank 0] Step 760/2500, Loss: 1.7435, LR: 4.99e-05\n[Rank 1] Step 760/2500, Loss: 1.7917, LR: 4.99e-05\n[Rank 1] Step 800/2500, Loss: 1.7803, LR: 4.96e-05[Rank 0] Step 800/2500, Loss: 1.7346, LR: 4.96e-05\n\n[Rank 0] Step 840/2500, Loss: 1.7227, LR: 4.93e-05\n[Rank 1] Step 840/2500, Loss: 1.7695, LR: 4.93e-05\n[Rank 0] Step 880/2500, Loss: 1.7199, LR: 4.90e-05[Rank 1] Step 880/2500, Loss: 1.7626, LR: 4.90e-05\n\n[Rank 0] Step 920/2500, Loss: 1.7077, LR: 4.87e-05\n[Rank 1] Step 920/2500, Loss: 1.7521, LR: 4.87e-05\n[Rank 0] Step 960/2500, Loss: 1.7035, LR: 4.84e-05[Rank 1] Step 960/2500, Loss: 1.7388, LR: 4.84e-05\n\n[Rank 0] Step 1000/2500, Loss: 1.6935, LR: 4.81e-05\n[Rank 1] Step 1000/2500, Loss: 1.7318, LR: 4.81e-05\n[Rank 1] Step 1040/2500, Loss: 1.7182, LR: 4.78e-05[Rank 0] Step 1040/2500, Loss: 1.6879, LR: 4.78e-05\n\n[Rank 0] Step 1080/2500, Loss: 1.6805, LR: 4.75e-05[Rank 1] Step 1080/2500, Loss: 1.7138, LR: 4.75e-05\n\n[Rank 1] Step 1120/2500, Loss: 1.7072, LR: 4.72e-05[Rank 0] Step 1120/2500, Loss: 1.6800, LR: 4.72e-05\n\n[Rank 0] Step 1160/2500, Loss: 1.6740, LR: 4.69e-05\n[Rank 1] Step 1160/2500, Loss: 1.7042, LR: 4.69e-05\n[Rank 1] Step 1200/2500, Loss: 1.6980, LR: 4.66e-05[Rank 0] Step 1200/2500, Loss: 1.6660, LR: 4.66e-05\n\n[Rank 1] Step 1240/2500, Loss: 1.6896, LR: 4.63e-05[Rank 0] Step 1240/2500, Loss: 1.6575, LR: 4.63e-05\n\n[Rank 0] Step 1280/2500, Loss: 1.6528, LR: 4.60e-05\n[Rank 1] Step 1280/2500, Loss: 1.6804, LR: 4.60e-05\n[Rank 0] Step 1320/2500, Loss: 1.6502, LR: 4.57e-05[Rank 1] Step 1320/2500, Loss: 1.6738, LR: 4.57e-05\n\n[Rank 0] Step 1360/2500, Loss: 1.6495, LR: 4.54e-05[Rank 1] Step 1360/2500, Loss: 1.6703, LR: 4.54e-05\n\n[Rank 0] Step 1400/2500, Loss: 1.6470, LR: 4.51e-05\n[Rank 1] Step 1400/2500, Loss: 1.6677, LR: 4.51e-05\n[Rank 0] Step 1440/2500, Loss: 1.6412, LR: 4.48e-05[Rank 1] Step 1440/2500, Loss: 1.6609, LR: 4.48e-05\n\n[Rank 0] Step 1480/2500, Loss: 1.6382, LR: 4.45e-05[Rank 1] Step 1480/2500, Loss: 1.6538, LR: 4.45e-05\n\n[Rank 0] Step 1520/2500, Loss: 1.6353, LR: 4.43e-05[Rank 1] Step 1520/2500, Loss: 1.6521, LR: 4.43e-05\n\n[Rank 0] Step 1560/2500, Loss: 1.6299, LR: 4.40e-05\n[Rank 1] Step 1560/2500, Loss: 1.6514, LR: 4.40e-05\n[Rank 1] Step 1600/2500, Loss: 1.6462, LR: 4.37e-05[Rank 0] Step 1600/2500, Loss: 1.6257, LR: 4.37e-05\n\n[Rank 0] Step 1640/2500, Loss: 1.6269, LR: 4.34e-05[Rank 1] Step 1640/2500, Loss: 1.6402, LR: 4.34e-05\n\n[Rank 1] Step 1680/2500, Loss: 1.6346, LR: 4.31e-05[Rank 0] Step 1680/2500, Loss: 1.6217, LR: 4.31e-05\n\n[Rank 0] Step 1720/2500, Loss: 1.6205, LR: 4.28e-05[Rank 1] Step 1720/2500, Loss: 1.6306, LR: 4.28e-05\n\n[Rank 0] Step 1760/2500, Loss: 1.6187, LR: 4.25e-05\n[Rank 1] Step 1760/2500, Loss: 1.6273, LR: 4.25e-05\n[Rank 0] Step 1800/2500, Loss: 1.6167, LR: 4.22e-05\n[Rank 1] Step 1800/2500, Loss: 1.6257, LR: 4.22e-05\n[Rank 0] Step 1840/2500, Loss: 1.6132, LR: 4.19e-05[Rank 1] Step 1840/2500, Loss: 1.6269, LR: 4.19e-05\n\n[Rank 0] Step 1880/2500, Loss: 1.6074, LR: 4.16e-05\n[Rank 1] Step 1880/2500, Loss: 1.6226, LR: 4.16e-05\n[Rank 0] Step 1920/2500, Loss: 1.6047, LR: 4.13e-05\n[Rank 1] Step 1920/2500, Loss: 1.6188, LR: 4.13e-05\n[Rank 0] Step 1960/2500, Loss: 1.6010, LR: 4.10e-05[Rank 1] Step 1960/2500, Loss: 1.6131, LR: 4.10e-05\n\n[Rank 0] Step 2000/2500, Loss: 1.5972, LR: 4.07e-05\n[Rank 1] Step 2000/2500, Loss: 1.6098, LR: 4.07e-05\n[Rank 0] Step 2040/2500, Loss: 1.5944, LR: 4.04e-05[Rank 1] Step 2040/2500, Loss: 1.6053, LR: 4.04e-05\n\n[Rank 0] Step 2080/2500, Loss: 1.5901, LR: 4.01e-05\n[Rank 1] Step 2080/2500, Loss: 1.6055, LR: 4.01e-05\n[Rank 0] Step 2120/2500, Loss: 1.5873, LR: 3.98e-05[Rank 1] Step 2120/2500, Loss: 1.6025, LR: 3.98e-05\n\n[Rank 0] Step 2160/2500, Loss: 1.5843, LR: 3.95e-05\n[Rank 1] Step 2160/2500, Loss: 1.5993, LR: 3.95e-05\n[Rank 0] Step 2200/2500, Loss: 1.5847, LR: 3.92e-05\n[Rank 1] Step 2200/2500, Loss: 1.5930, LR: 3.92e-05\n[Rank 0] Step 2240/2500, Loss: 1.5810, LR: 3.89e-05[Rank 1] Step 2240/2500, Loss: 1.5901, LR: 3.89e-05\n\n[Rank 0] Step 2280/2500, Loss: 1.5793, LR: 3.86e-05\n[Rank 1] Step 2280/2500, Loss: 1.5875, LR: 3.86e-05\n[Rank 0] Step 2320/2500, Loss: 1.5782, LR: 3.83e-05\n[Rank 1] Step 2320/2500, Loss: 1.5886, LR: 3.83e-05\n[Rank 1] Step 2360/2500, Loss: 1.5839, LR: 3.80e-05\n[Rank 0] Step 2360/2500, Loss: 1.5775, LR: 3.80e-05\n[Rank 0] Step 2400/2500, Loss: 1.5732, LR: 3.77e-05[Rank 1] Step 2400/2500, Loss: 1.5792, LR: 3.77e-05\n\n[Rank 1] Step 2440/2500, Loss: 1.5746, LR: 3.74e-05\n[Rank 0] Step 2440/2500, Loss: 1.5726, LR: 3.74e-05\n[Rank 0] Step 2480/2500, Loss: 1.5689, LR: 3.71e-05\n[Rank 1] Step 2480/2500, Loss: 1.5720, LR: 3.71e-05\nTraining Time: 41 mins 32 seconds\nTraining Loss: 1.5680\nValidating...\n[Rank 0] Step 40/2500, Loss: 1.4497[Rank 1] Step 40/2500, Loss: 1.5924\n\n[Rank 0] Step 80/2500, Loss: 1.3894\n[Rank 1] Step 80/2500, Loss: 1.5093\n[Rank 0] Step 120/2500, Loss: 1.4172[Rank 1] Step 120/2500, Loss: 1.5283\n\n[Rank 1] Step 160/2500, Loss: 1.5234[Rank 0] Step 160/2500, Loss: 1.4305\n\n[Rank 1] Step 200/2500, Loss: 1.5003[Rank 0] Step 200/2500, Loss: 1.4361\n\n[Rank 1] Step 240/2500, Loss: 1.4865[Rank 0] Step 240/2500, Loss: 1.4219\n\n[Rank 0] Step 280/2500, Loss: 1.4288[Rank 1] Step 280/2500, Loss: 1.4811\n\n[Rank 1] Step 320/2500, Loss: 1.4544[Rank 0] Step 320/2500, Loss: 1.4098\n\n[Rank 0] Step 360/2500, Loss: 1.3922[Rank 1] Step 360/2500, Loss: 1.4420\n\n[Rank 0] Step 400/2500, Loss: 1.4042[Rank 1] Step 400/2500, Loss: 1.4249\n\n[Rank 1] Step 440/2500, Loss: 1.4147[Rank 0] Step 440/2500, Loss: 1.4004\n\n[Rank 1] Step 480/2500, Loss: 1.4148[Rank 0] Step 480/2500, Loss: 1.3853\n\n[Rank 0] Step 520/2500, Loss: 1.3894[Rank 1] Step 520/2500, Loss: 1.4287\n\n[Rank 0] Step 560/2500, Loss: 1.3923[Rank 1] Step 560/2500, Loss: 1.4213\n\n[Rank 1] Step 600/2500, Loss: 1.4256[Rank 0] Step 600/2500, Loss: 1.3937\n\n[Rank 0] Step 640/2500, Loss: 1.3938[Rank 1] Step 640/2500, Loss: 1.4354\n\n[Rank 1] Step 680/2500, Loss: 1.4384[Rank 0] Step 680/2500, Loss: 1.4025\n\n[Rank 1] Step 720/2500, Loss: 1.4443[Rank 0] Step 720/2500, Loss: 1.4088\n\n[Rank 0] Step 760/2500, Loss: 1.4045[Rank 1] Step 760/2500, Loss: 1.4512\n\n[Rank 0] Step 800/2500, Loss: 1.4082[Rank 1] Step 800/2500, Loss: 1.4497\n\n[Rank 1] Step 840/2500, Loss: 1.4489[Rank 0] Step 840/2500, Loss: 1.4085\n\n[Rank 1] Step 880/2500, Loss: 1.4371[Rank 0] Step 880/2500, Loss: 1.3979\n\n[Rank 0] Step 920/2500, Loss: 1.4006[Rank 1] Step 920/2500, Loss: 1.4386\n\n[Rank 0] Step 960/2500, Loss: 1.4002[Rank 1] Step 960/2500, Loss: 1.4325\n\n[Rank 0] Step 1000/2500, Loss: 1.4059[Rank 1] Step 1000/2500, Loss: 1.4314\n\n[Rank 1] Step 1040/2500, Loss: 1.4304[Rank 0] Step 1040/2500, Loss: 1.4023\n\n[Rank 0] Step 1080/2500, Loss: 1.3997[Rank 1] Step 1080/2500, Loss: 1.4290\n\n[Rank 1] Step 1120/2500, Loss: 1.4290[Rank 0] Step 1120/2500, Loss: 1.4008\n\n[Rank 0] Step 1160/2500, Loss: 1.3961[Rank 1] Step 1160/2500, Loss: 1.4318\n\n[Rank 1] Step 1200/2500, Loss: 1.4355[Rank 0] Step 1200/2500, Loss: 1.3964\n\n[Rank 0] Step 1240/2500, Loss: 1.3957[Rank 1] Step 1240/2500, Loss: 1.4299\n\n[Rank 0] Step 1280/2500, Loss: 1.3941[Rank 1] Step 1280/2500, Loss: 1.4315\n\n[Rank 1] Step 1320/2500, Loss: 1.4298[Rank 0] Step 1320/2500, Loss: 1.3943\n\n[Rank 0] Step 1360/2500, Loss: 1.3932[Rank 1] Step 1360/2500, Loss: 1.4274\n\n[Rank 0] Step 1400/2500, Loss: 1.3945[Rank 1] Step 1400/2500, Loss: 1.4304\n\n[Rank 1] Step 1440/2500, Loss: 1.4290[Rank 0] Step 1440/2500, Loss: 1.3900\n\n[Rank 0] Step 1480/2500, Loss: 1.3920[Rank 1] Step 1480/2500, Loss: 1.4303\n\n[Rank 1] Step 1520/2500, Loss: 1.4317[Rank 0] Step 1520/2500, Loss: 1.3928\n\n[Rank 1] Step 1560/2500, Loss: 1.4321[Rank 0] Step 1560/2500, Loss: 1.3947\n\n[Rank 0] Step 1600/2500, Loss: 1.3974[Rank 1] Step 1600/2500, Loss: 1.4319\n\n[Rank 0] Step 1640/2500, Loss: 1.3974[Rank 1] Step 1640/2500, Loss: 1.4321\n\n[Rank 0] Step 1680/2500, Loss: 1.3983[Rank 1] Step 1680/2500, Loss: 1.4295\n\n[Rank 0] Step 1720/2500, Loss: 1.4006[Rank 1] Step 1720/2500, Loss: 1.4287\n\n[Rank 1] Step 1760/2500, Loss: 1.4313[Rank 0] Step 1760/2500, Loss: 1.4024\n\n[Rank 0] Step 1800/2500, Loss: 1.4000\n[Rank 1] Step 1800/2500, Loss: 1.4329\n[Rank 1] Step 1840/2500, Loss: 1.4299[Rank 0] Step 1840/2500, Loss: 1.3981\n\n[Rank 0] Step 1880/2500, Loss: 1.3970[Rank 1] Step 1880/2500, Loss: 1.4275\n\n[Rank 1] Step 1920/2500, Loss: 1.4278[Rank 0] Step 1920/2500, Loss: 1.3961\n\n[Rank 0] Step 1960/2500, Loss: 1.3982[Rank 1] Step 1960/2500, Loss: 1.4277\n\n[Rank 0] Step 2000/2500, Loss: 1.3969[Rank 1] Step 2000/2500, Loss: 1.4264\n\n[Rank 1] Step 2040/2500, Loss: 1.4237[Rank 0] Step 2040/2500, Loss: 1.3957\n\n[Rank 1] Step 2080/2500, Loss: 1.4227[Rank 0] Step 2080/2500, Loss: 1.3980\n\n[Rank 0] Step 2120/2500, Loss: 1.3979[Rank 1] Step 2120/2500, Loss: 1.4214\n\n[Rank 0] Step 2160/2500, Loss: 1.3942[Rank 1] Step 2160/2500, Loss: 1.4205\n\n[Rank 0] Step 2200/2500, Loss: 1.3976[Rank 1] Step 2200/2500, Loss: 1.4209\n\n[Rank 0] Step 2240/2500, Loss: 1.3966[Rank 1] Step 2240/2500, Loss: 1.4197\n\n[Rank 0] Step 2280/2500, Loss: 1.3939[Rank 1] Step 2280/2500, Loss: 1.4229\n\n[Rank 0] Step 2320/2500, Loss: 1.3949[Rank 1] Step 2320/2500, Loss: 1.4249\n\n[Rank 0] Step 2360/2500, Loss: 1.3960[Rank 1] Step 2360/2500, Loss: 1.4260\n\n[Rank 1] Step 2400/2500, Loss: 1.4298[Rank 0] Step 2400/2500, Loss: 1.3959\n\n[Rank 1] Step 2440/2500, Loss: 1.4274[Rank 0] Step 2440/2500, Loss: 1.3934\n\n[Rank 0] Step 2480/2500, Loss: 1.3950[Rank 1] Step 2480/2500, Loss: 1.4256\n\nValidation Time: 10 mins 5 seconds\nValidation Loss: 1.3963\nPerplexity: 4.04\n\n-------------------------\nBest save mode\n-------------------------\n/usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n[Rank 1] Done writing model state dict\n[Rank 0] Done writing model state dict\nSaving model...\nNew best model saved to ./qwen-vietnamese-wiki-finetuned/best-model-ckpt.pt\nbest-model-ckpt.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.01G/3.01G [01:01<00:00, 48.5MB/s]\nAlso saved to Huggingface repo Quoc59/PARADIS-Qwen3_0.6B-10kWikiVi-FSDP\n\n-------------------------\nRegular save mode\n-------------------------\n[Rank 1] Done writing model state dict\n[Rank 0] Done writing model state dict\nSaving model...\nModel at epoch 0 saved to ./qwen-vietnamese-wiki-finetuned/last-model-ckpt.pt\nlast-model-ckpt.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.01G/3.01G [00:57<00:00, 52.4MB/s]\nAlso saved to Huggingface repo Quoc59/PARADIS-Qwen3_0.6B-10kWikiVi-FSDP\n\n==================================================\nEpoch 2/3\n==================================================\nTraining...\n[Rank 1] Step 40/2500, Loss: 1.4535, LR: 3.67e-05[Rank 0] Step 40/2500, Loss: 1.3607, LR: 3.67e-05\n\n[Rank 0] Step 80/2500, Loss: 1.2930, LR: 3.64e-05[Rank 1] Step 80/2500, Loss: 1.3723, LR: 3.64e-05\n\n[Rank 1] Step 120/2500, Loss: 1.3167, LR: 3.61e-05[Rank 0] Step 120/2500, Loss: 1.3201, LR: 3.61e-05\n\n[Rank 1] Step 160/2500, Loss: 1.3307, LR: 3.58e-05[Rank 0] Step 160/2500, Loss: 1.3262, LR: 3.58e-05\n\n[Rank 1] Step 200/2500, Loss: 1.3610, LR: 3.55e-05[Rank 0] Step 200/2500, Loss: 1.3062, LR: 3.55e-05\n\n[Rank 0] Step 240/2500, Loss: 1.2945, LR: 3.52e-05\n[Rank 1] Step 240/2500, Loss: 1.3407, LR: 3.52e-05\n[Rank 0] Step 280/2500, Loss: 1.2828, LR: 3.50e-05[Rank 1] Step 280/2500, Loss: 1.3174, LR: 3.50e-05\n\n[Rank 0] Step 320/2500, Loss: 1.2480, LR: 3.47e-05\n[Rank 1] Step 320/2500, Loss: 1.3039, LR: 3.47e-05\n[Rank 1] Step 360/2500, Loss: 1.3081, LR: 3.44e-05[Rank 0] Step 360/2500, Loss: 1.2326, LR: 3.44e-05\n\n[Rank 1] Step 400/2500, Loss: 1.2888, LR: 3.41e-05[Rank 0] Step 400/2500, Loss: 1.2361, LR: 3.41e-05\n\n[Rank 0] Step 440/2500, Loss: 1.2178, LR: 3.38e-05\n[Rank 1] Step 440/2500, Loss: 1.2811, LR: 3.38e-05\n[Rank 0] Step 480/2500, Loss: 1.2092, LR: 3.35e-05\n[Rank 1] Step 480/2500, Loss: 1.2646, LR: 3.35e-05\n[Rank 1] Step 520/2500, Loss: 1.2552, LR: 3.32e-05[Rank 0] Step 520/2500, Loss: 1.1956, LR: 3.32e-05\n\n[Rank 0] Step 560/2500, Loss: 1.1871, LR: 3.29e-05[Rank 1] Step 560/2500, Loss: 1.2411, LR: 3.29e-05\n\n[Rank 0] Step 600/2500, Loss: 1.1727, LR: 3.26e-05\n[Rank 1] Step 600/2500, Loss: 1.2397, LR: 3.26e-05\n[Rank 1] Step 640/2500, Loss: 1.2233, LR: 3.23e-05[Rank 0] Step 640/2500, Loss: 1.1679, LR: 3.23e-05\n\n[Rank 0] Step 680/2500, Loss: 1.1588, LR: 3.20e-05[Rank 1] Step 680/2500, Loss: 1.2147, LR: 3.20e-05\n\n[Rank 0] Step 720/2500, Loss: 1.1569, LR: 3.17e-05[Rank 1] Step 720/2500, Loss: 1.2071, LR: 3.17e-05\n\n[Rank 0] Step 760/2500, Loss: 1.1498, LR: 3.14e-05[Rank 1] Step 760/2500, Loss: 1.1985, LR: 3.14e-05\n\n[Rank 0] Step 800/2500, Loss: 1.1454, LR: 3.11e-05[Rank 1] Step 800/2500, Loss: 1.1922, LR: 3.11e-05\n\n[Rank 0] Step 840/2500, Loss: 1.1373, LR: 3.08e-05\n[Rank 1] Step 840/2500, Loss: 1.1847, LR: 3.08e-05\n[Rank 0] Step 880/2500, Loss: 1.1356, LR: 3.05e-05\n[Rank 1] Step 880/2500, Loss: 1.1800, LR: 3.05e-05\n[Rank 0] Step 920/2500, Loss: 1.1287, LR: 3.02e-05\n[Rank 1] Step 920/2500, Loss: 1.1724, LR: 3.02e-05\n[Rank 0] Step 960/2500, Loss: 1.1261, LR: 2.99e-05[Rank 1] Step 960/2500, Loss: 1.1649, LR: 2.99e-05\n\n[Rank 0] Step 1000/2500, Loss: 1.1204, LR: 2.96e-05\n[Rank 1] Step 1000/2500, Loss: 1.1601, LR: 2.96e-05\n[Rank 0] Step 1040/2500, Loss: 1.1176, LR: 2.93e-05\n[Rank 1] Step 1040/2500, Loss: 1.1516, LR: 2.93e-05\n[Rank 0] Step 1080/2500, Loss: 1.1133, LR: 2.90e-05[Rank 1] Step 1080/2500, Loss: 1.1495, LR: 2.90e-05\n\n[Rank 1] Step 1120/2500, Loss: 1.1455, LR: 2.87e-05[Rank 0] Step 1120/2500, Loss: 1.1147, LR: 2.87e-05\n\n[Rank 0] Step 1160/2500, Loss: 1.1113, LR: 2.84e-05\n[Rank 1] Step 1160/2500, Loss: 1.1442, LR: 2.84e-05\n[Rank 0] Step 1200/2500, Loss: 1.1063, LR: 2.81e-05\n[Rank 1] Step 1200/2500, Loss: 1.1407, LR: 2.81e-05\n[Rank 1] Step 1240/2500, Loss: 1.1360, LR: 2.78e-05[Rank 0] Step 1240/2500, Loss: 1.1008, LR: 2.78e-05\n\n[Rank 0] Step 1280/2500, Loss: 1.0988, LR: 2.75e-05\n[Rank 1] Step 1280/2500, Loss: 1.1302, LR: 2.75e-05\n[Rank 0] Step 1320/2500, Loss: 1.0976, LR: 2.73e-05\n[Rank 1] Step 1320/2500, Loss: 1.1257, LR: 2.73e-05\n[Rank 0] Step 1360/2500, Loss: 1.0978, LR: 2.70e-05[Rank 1] Step 1360/2500, Loss: 1.1238, LR: 2.70e-05\n\n[Rank 0] Step 1400/2500, Loss: 1.0970, LR: 2.67e-05[Rank 1] Step 1400/2500, Loss: 1.1228, LR: 2.67e-05\n\n[Rank 1] Step 1440/2500, Loss: 1.1184, LR: 2.64e-05[Rank 0] Step 1440/2500, Loss: 1.0939, LR: 2.64e-05\n\n[Rank 0] Step 1480/2500, Loss: 1.0928, LR: 2.61e-05[Rank 1] Step 1480/2500, Loss: 1.1132, LR: 2.61e-05\n\n[Rank 0] Step 1520/2500, Loss: 1.0918, LR: 2.58e-05[Rank 1] Step 1520/2500, Loss: 1.1123, LR: 2.58e-05\n\n[Rank 1] Step 1560/2500, Loss: 1.1120, LR: 2.55e-05[Rank 0] Step 1560/2500, Loss: 1.0893, LR: 2.55e-05\n\n[Rank 0] Step 1600/2500, Loss: 1.0871, LR: 2.52e-05\n[Rank 1] Step 1600/2500, Loss: 1.1089, LR: 2.52e-05\n[Rank 0] Step 1640/2500, Loss: 1.0886, LR: 2.49e-05[Rank 1] Step 1640/2500, Loss: 1.1053, LR: 2.49e-05\n\n[Rank 1] Step 1680/2500, Loss: 1.1023, LR: 2.46e-05[Rank 0] Step 1680/2500, Loss: 1.0859, LR: 2.46e-05\n\n[Rank 1] Step 1720/2500, Loss: 1.1001, LR: 2.43e-05[Rank 0] Step 1720/2500, Loss: 1.0859, LR: 2.43e-05\n\n[Rank 0] Step 1760/2500, Loss: 1.0846, LR: 2.40e-05\n[Rank 1] Step 1760/2500, Loss: 1.0984, LR: 2.40e-05\n[Rank 0] Step 1800/2500, Loss: 1.0836, LR: 2.37e-05\n[Rank 1] Step 1800/2500, Loss: 1.0985, LR: 2.37e-05\n[Rank 0] Step 1840/2500, Loss: 1.0821, LR: 2.34e-05[Rank 1] Step 1840/2500, Loss: 1.0994, LR: 2.34e-05\n\n[Rank 1] Step 1880/2500, Loss: 1.0965, LR: 2.31e-05[Rank 0] Step 1880/2500, Loss: 1.0788, LR: 2.31e-05\n\n[Rank 0] Step 1920/2500, Loss: 1.0772, LR: 2.28e-05[Rank 1] Step 1920/2500, Loss: 1.0941, LR: 2.28e-05\n\n[Rank 0] Step 1960/2500, Loss: 1.0754, LR: 2.25e-05[Rank 1] Step 1960/2500, Loss: 1.0904, LR: 2.25e-05\n\n[Rank 0] Step 2000/2500, Loss: 1.0734, LR: 2.22e-05[Rank 1] Step 2000/2500, Loss: 1.0889, LR: 2.22e-05\n\n[Rank 0] Step 2040/2500, Loss: 1.0718, LR: 2.19e-05\n[Rank 1] Step 2040/2500, Loss: 1.0859, LR: 2.19e-05\n[Rank 1] Step 2080/2500, Loss: 1.0864, LR: 2.16e-05[Rank 0] Step 2080/2500, Loss: 1.0692, LR: 2.16e-05\n\n[Rank 0] Step 2120/2500, Loss: 1.0681, LR: 2.13e-05\n[Rank 1] Step 2120/2500, Loss: 1.0849, LR: 2.13e-05\n[Rank 0] Step 2160/2500, Loss: 1.0665, LR: 2.10e-05[Rank 1] Step 2160/2500, Loss: 1.0833, LR: 2.10e-05\n\n[Rank 0] Step 2200/2500, Loss: 1.0671, LR: 2.07e-05[Rank 1] Step 2200/2500, Loss: 1.0795, LR: 2.07e-05\n\n[Rank 0] Step 2240/2500, Loss: 1.0650, LR: 2.04e-05[Rank 1] Step 2240/2500, Loss: 1.0781, LR: 2.04e-05\n\n[Rank 1] Step 2280/2500, Loss: 1.0767, LR: 2.01e-05[Rank 0] Step 2280/2500, Loss: 1.0646, LR: 2.01e-05\n\n[Rank 1] Step 2320/2500, Loss: 1.0777, LR: 1.98e-05[Rank 0] Step 2320/2500, Loss: 1.0647, LR: 1.98e-05\n\n[Rank 0] Step 2360/2500, Loss: 1.0642, LR: 1.95e-05\n[Rank 1] Step 2360/2500, Loss: 1.0752, LR: 1.95e-05\n[Rank 0] Step 2400/2500, Loss: 1.0617, LR: 1.93e-05[Rank 1] Step 2400/2500, Loss: 1.0725, LR: 1.93e-05\n\n[Rank 0] Step 2440/2500, Loss: 1.0621, LR: 1.90e-05[Rank 1] Step 2440/2500, Loss: 1.0699, LR: 1.90e-05\n\n[Rank 0] Step 2480/2500, Loss: 1.0603, LR: 1.87e-05\n[Rank 1] Step 2480/2500, Loss: 1.0683, LR: 1.87e-05\nTraining Time: 41 mins 27 seconds\nTraining Loss: 1.0607\nValidating...\n[Rank 1] Step 40/2500, Loss: 1.5514[Rank 0] Step 40/2500, Loss: 1.3847\n\n[Rank 1] Step 80/2500, Loss: 1.4766[Rank 0] Step 80/2500, Loss: 1.3324\n\n[Rank 0] Step 120/2500, Loss: 1.3687[Rank 1] Step 120/2500, Loss: 1.5078\n\n[Rank 0] Step 160/2500, Loss: 1.3845[Rank 1] Step 160/2500, Loss: 1.4997\n\n[Rank 1] Step 200/2500, Loss: 1.4726[Rank 0] Step 200/2500, Loss: 1.3914\n\n[Rank 0] Step 240/2500, Loss: 1.3796\n[Rank 1] Step 240/2500, Loss: 1.4589\n[Rank 1] Step 280/2500, Loss: 1.4529[Rank 0] Step 280/2500, Loss: 1.3896\n\n[Rank 1] Step 320/2500, Loss: 1.4231[Rank 0] Step 320/2500, Loss: 1.3679\n\n[Rank 0] Step 360/2500, Loss: 1.3475[Rank 1] Step 360/2500, Loss: 1.4121\n\n[Rank 0] Step 400/2500, Loss: 1.3617[Rank 1] Step 400/2500, Loss: 1.3954\n\n[Rank 0] Step 440/2500, Loss: 1.3590[Rank 1] Step 440/2500, Loss: 1.3835\n\n[Rank 0] Step 480/2500, Loss: 1.3449[Rank 1] Step 480/2500, Loss: 1.3842\n\n[Rank 0] Step 520/2500, Loss: 1.3504[Rank 1] Step 520/2500, Loss: 1.3985\n\n[Rank 0] Step 560/2500, Loss: 1.3548[Rank 1] Step 560/2500, Loss: 1.3907\n\n[Rank 0] Step 600/2500, Loss: 1.3557[Rank 1] Step 600/2500, Loss: 1.3932\n\n[Rank 1] Step 640/2500, Loss: 1.4040\n[Rank 0] Step 640/2500, Loss: 1.3559\n[Rank 0] Step 680/2500, Loss: 1.3663\n[Rank 1] Step 680/2500, Loss: 1.4089\n[Rank 0] Step 720/2500, Loss: 1.3736[Rank 1] Step 720/2500, Loss: 1.4151\n\n[Rank 0] Step 760/2500, Loss: 1.3692[Rank 1] Step 760/2500, Loss: 1.4209\n\n[Rank 0] Step 800/2500, Loss: 1.3738\n[Rank 1] Step 800/2500, Loss: 1.4195\n[Rank 0] Step 840/2500, Loss: 1.3746[Rank 1] Step 840/2500, Loss: 1.4176\n\n[Rank 1] Step 880/2500, Loss: 1.4065[Rank 0] Step 880/2500, Loss: 1.3639\n\n[Rank 0] Step 920/2500, Loss: 1.3659\n[Rank 1] Step 920/2500, Loss: 1.4077\n[Rank 0] Step 960/2500, Loss: 1.3647[Rank 1] Step 960/2500, Loss: 1.4005\n\n[Rank 0] Step 1000/2500, Loss: 1.3706[Rank 1] Step 1000/2500, Loss: 1.3993\n\n[Rank 0] Step 1040/2500, Loss: 1.3664[Rank 1] Step 1040/2500, Loss: 1.3984\n\n[Rank 0] Step 1080/2500, Loss: 1.3638[Rank 1] Step 1080/2500, Loss: 1.3966\n\n[Rank 0] Step 1120/2500, Loss: 1.3650[Rank 1] Step 1120/2500, Loss: 1.3956\n\n[Rank 0] Step 1160/2500, Loss: 1.3610[Rank 1] Step 1160/2500, Loss: 1.3989\n\n[Rank 1] Step 1200/2500, Loss: 1.4026[Rank 0] Step 1200/2500, Loss: 1.3607\n\n[Rank 1] Step 1240/2500, Loss: 1.3968[Rank 0] Step 1240/2500, Loss: 1.3601\n\n[Rank 1] Step 1280/2500, Loss: 1.3987[Rank 0] Step 1280/2500, Loss: 1.3583\n\n[Rank 1] Step 1320/2500, Loss: 1.3976[Rank 0] Step 1320/2500, Loss: 1.3584\n\n[Rank 1] Step 1360/2500, Loss: 1.3951[Rank 0] Step 1360/2500, Loss: 1.3576\n\n[Rank 1] Step 1400/2500, Loss: 1.3989[Rank 0] Step 1400/2500, Loss: 1.3596\n\n[Rank 1] Step 1440/2500, Loss: 1.3979[Rank 0] Step 1440/2500, Loss: 1.3551\n\n[Rank 0] Step 1480/2500, Loss: 1.3573[Rank 1] Step 1480/2500, Loss: 1.3992\n\n[Rank 1] Step 1520/2500, Loss: 1.4011[Rank 0] Step 1520/2500, Loss: 1.3576\n\n[Rank 1] Step 1560/2500, Loss: 1.4012\n[Rank 0] Step 1560/2500, Loss: 1.3602\n[Rank 0] Step 1600/2500, Loss: 1.3633[Rank 1] Step 1600/2500, Loss: 1.4009\n\n[Rank 1] Step 1640/2500, Loss: 1.4005[Rank 0] Step 1640/2500, Loss: 1.3639\n\n[Rank 1] Step 1680/2500, Loss: 1.3972\n[Rank 0] Step 1680/2500, Loss: 1.3646\n[Rank 0] Step 1720/2500, Loss: 1.3672\n[Rank 1] Step 1720/2500, Loss: 1.3963\n[Rank 1] Step 1760/2500, Loss: 1.3992[Rank 0] Step 1760/2500, Loss: 1.3690\n\n[Rank 1] Step 1800/2500, Loss: 1.4009[Rank 0] Step 1800/2500, Loss: 1.3666\n\n[Rank 1] Step 1840/2500, Loss: 1.3977[Rank 0] Step 1840/2500, Loss: 1.3651\n\n[Rank 0] Step 1880/2500, Loss: 1.3631[Rank 1] Step 1880/2500, Loss: 1.3951\n\n[Rank 1] Step 1920/2500, Loss: 1.3959[Rank 0] Step 1920/2500, Loss: 1.3623\n\n[Rank 1] Step 1960/2500, Loss: 1.3955[Rank 0] Step 1960/2500, Loss: 1.3648\n\n[Rank 0] Step 2000/2500, Loss: 1.3633[Rank 1] Step 2000/2500, Loss: 1.3940\n\n[Rank 1] Step 2040/2500, Loss: 1.3915[Rank 0] Step 2040/2500, Loss: 1.3623\n\n[Rank 0] Step 2080/2500, Loss: 1.3649[Rank 1] Step 2080/2500, Loss: 1.3907\n\n[Rank 1] Step 2120/2500, Loss: 1.3890[Rank 0] Step 2120/2500, Loss: 1.3645\n\n[Rank 0] Step 2160/2500, Loss: 1.3609[Rank 1] Step 2160/2500, Loss: 1.3885\n\n[Rank 1] Step 2200/2500, Loss: 1.3890[Rank 0] Step 2200/2500, Loss: 1.3644\n\n[Rank 0] Step 2240/2500, Loss: 1.3630[Rank 1] Step 2240/2500, Loss: 1.3880\n\n[Rank 0] Step 2280/2500, Loss: 1.3604[Rank 1] Step 2280/2500, Loss: 1.3912\n\n[Rank 1] Step 2320/2500, Loss: 1.3932[Rank 0] Step 2320/2500, Loss: 1.3615\n\n[Rank 0] Step 2360/2500, Loss: 1.3623\n[Rank 1] Step 2360/2500, Loss: 1.3943\n[Rank 1] Step 2400/2500, Loss: 1.3981[Rank 0] Step 2400/2500, Loss: 1.3623\n\n[Rank 0] Step 2440/2500, Loss: 1.3600[Rank 1] Step 2440/2500, Loss: 1.3958\n\n[Rank 1] Step 2480/2500, Loss: 1.3939\n[Rank 0] Step 2480/2500, Loss: 1.3618\nValidation Time: 10 mins 6 seconds\nValidation Loss: 1.3631\nPerplexity: 3.91\n\n-------------------------\nBest save mode\n-------------------------\n[Rank 1] Done writing model state dict\n[Rank 0] Done writing model state dict\nSaving model...\nNew best model saved to ./qwen-vietnamese-wiki-finetuned/best-model-ckpt.pt\nbest-model-ckpt.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.01G/3.01G [00:55<00:00, 54.4MB/s]\nAlso saved to Huggingface repo Quoc59/PARADIS-Qwen3_0.6B-10kWikiVi-FSDP\n\n-------------------------\nRegular save mode\n-------------------------\n[Rank 1] Done writing model state dict\n[Rank 0] Done writing model state dict\nSaving model...\nModel at epoch 1 saved to ./qwen-vietnamese-wiki-finetuned/last-model-ckpt.pt\nlast-model-ckpt.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.01G/3.01G [00:56<00:00, 53.4MB/s]\nAlso saved to Huggingface repo Quoc59/PARADIS-Qwen3_0.6B-10kWikiVi-FSDP\n\n==================================================\nEpoch 3/3\n==================================================\nTraining...\n[Rank 1] Step 40/2500, Loss: 0.9998, LR: 1.82e-05[Rank 0] Step 40/2500, Loss: 0.9657, LR: 1.82e-05\n\n[Rank 1] Step 80/2500, Loss: 0.9454, LR: 1.80e-05[Rank 0] Step 80/2500, Loss: 0.9143, LR: 1.80e-05\n\n[Rank 1] Step 120/2500, Loss: 0.9134, LR: 1.77e-05\n[Rank 0] Step 120/2500, Loss: 0.9300, LR: 1.77e-05\n[Rank 1] Step 160/2500, Loss: 0.9225, LR: 1.74e-05[Rank 0] Step 160/2500, Loss: 0.9342, LR: 1.74e-05\n\n[Rank 1] Step 200/2500, Loss: 0.9427, LR: 1.71e-05[Rank 0] Step 200/2500, Loss: 0.9223, LR: 1.71e-05\n\n[Rank 1] Step 240/2500, Loss: 0.9298, LR: 1.68e-05[Rank 0] Step 240/2500, Loss: 0.9138, LR: 1.68e-05\n\n[Rank 1] Step 280/2500, Loss: 0.9167, LR: 1.65e-05[Rank 0] Step 280/2500, Loss: 0.9075, LR: 1.65e-05\n\n[Rank 0] Step 320/2500, Loss: 0.8811, LR: 1.62e-05\n[Rank 1] Step 320/2500, Loss: 0.9093, LR: 1.62e-05\n[Rank 0] Step 360/2500, Loss: 0.8700, LR: 1.59e-05\n[Rank 1] Step 360/2500, Loss: 0.9119, LR: 1.59e-05\n[Rank 0] Step 400/2500, Loss: 0.8692, LR: 1.56e-05\n[Rank 1] Step 400/2500, Loss: 0.9000, LR: 1.56e-05\n[Rank 1] Step 440/2500, Loss: 0.8930, LR: 1.53e-05[Rank 0] Step 440/2500, Loss: 0.8563, LR: 1.53e-05\n\n[Rank 0] Step 480/2500, Loss: 0.8485, LR: 1.50e-05\n[Rank 1] Step 480/2500, Loss: 0.8811, LR: 1.50e-05\n[Rank 1] Step 520/2500, Loss: 0.8736, LR: 1.47e-05\n[Rank 0] Step 520/2500, Loss: 0.8358, LR: 1.47e-05\n[Rank 0] Step 560/2500, Loss: 0.8300, LR: 1.44e-05[Rank 1] Step 560/2500, Loss: 0.8639, LR: 1.44e-05\n\n[Rank 0] Step 600/2500, Loss: 0.8209, LR: 1.41e-05\n[Rank 1] Step 600/2500, Loss: 0.8615, LR: 1.41e-05\n[Rank 1] Step 640/2500, Loss: 0.8489, LR: 1.38e-05[Rank 0] Step 640/2500, Loss: 0.8175, LR: 1.38e-05\n\n[Rank 0] Step 680/2500, Loss: 0.8113, LR: 1.35e-05\n[Rank 1] Step 680/2500, Loss: 0.8418, LR: 1.35e-05\n[Rank 0] Step 720/2500, Loss: 0.8093, LR: 1.32e-05\n[Rank 1] Step 720/2500, Loss: 0.8365, LR: 1.32e-05\n[Rank 1] Step 760/2500, Loss: 0.8299, LR: 1.29e-05[Rank 0] Step 760/2500, Loss: 0.8040, LR: 1.29e-05\n\n[Rank 0] Step 800/2500, Loss: 0.8004, LR: 1.26e-05[Rank 1] Step 800/2500, Loss: 0.8248, LR: 1.26e-05\n\n[Rank 0] Step 840/2500, Loss: 0.7932, LR: 1.23e-05\n[Rank 1] Step 840/2500, Loss: 0.8188, LR: 1.23e-05\n[Rank 0] Step 880/2500, Loss: 0.7902, LR: 1.20e-05[Rank 1] Step 880/2500, Loss: 0.8134, LR: 1.20e-05\n\n[Rank 0] Step 920/2500, Loss: 0.7852, LR: 1.17e-05\n[Rank 1] Step 920/2500, Loss: 0.8073, LR: 1.17e-05\n[Rank 0] Step 960/2500, Loss: 0.7820, LR: 1.14e-05\n[Rank 1] Step 960/2500, Loss: 0.8026, LR: 1.14e-05\n[Rank 1] Step 1000/2500, Loss: 0.7988, LR: 1.11e-05\n[Rank 0] Step 1000/2500, Loss: 0.7777, LR: 1.11e-05\n[Rank 0] Step 1040/2500, Loss: 0.7749, LR: 1.08e-05\n[Rank 1] Step 1040/2500, Loss: 0.7931, LR: 1.08e-05\n[Rank 0] Step 1080/2500, Loss: 0.7717, LR: 1.05e-05[Rank 1] Step 1080/2500, Loss: 0.7910, LR: 1.05e-05\n\n[Rank 0] Step 1120/2500, Loss: 0.7727, LR: 1.02e-05\n[Rank 1] Step 1120/2500, Loss: 0.7885, LR: 1.02e-05\n[Rank 1] Step 1160/2500, Loss: 0.7871, LR: 9.95e-06[Rank 0] Step 1160/2500, Loss: 0.7704, LR: 9.95e-06\n\n[Rank 0] Step 1200/2500, Loss: 0.7665, LR: 9.66e-06\n[Rank 1] Step 1200/2500, Loss: 0.7850, LR: 9.66e-06\n[Rank 0] Step 1240/2500, Loss: 0.7630, LR: 9.36e-06\n[Rank 1] Step 1240/2500, Loss: 0.7814, LR: 9.36e-06\n[Rank 0] Step 1280/2500, Loss: 0.7614, LR: 9.06e-06[Rank 1] Step 1280/2500, Loss: 0.7772, LR: 9.06e-06\n\n[Rank 1] Step 1320/2500, Loss: 0.7740, LR: 8.77e-06[Rank 0] Step 1320/2500, Loss: 0.7599, LR: 8.77e-06\n\n[Rank 1] Step 1360/2500, Loss: 0.7723, LR: 8.47e-06[Rank 0] Step 1360/2500, Loss: 0.7599, LR: 8.47e-06\n\n[Rank 1] Step 1400/2500, Loss: 0.7710, LR: 8.18e-06[Rank 0] Step 1400/2500, Loss: 0.7590, LR: 8.18e-06\n\n[Rank 0] Step 1440/2500, Loss: 0.7563, LR: 7.88e-06[Rank 1] Step 1440/2500, Loss: 0.7677, LR: 7.88e-06\n\n[Rank 0] Step 1480/2500, Loss: 0.7550, LR: 7.58e-06[Rank 1] Step 1480/2500, Loss: 0.7641, LR: 7.58e-06\n\n[Rank 0] Step 1520/2500, Loss: 0.7538, LR: 7.29e-06[Rank 1] Step 1520/2500, Loss: 0.7630, LR: 7.29e-06\n\n[Rank 0] Step 1560/2500, Loss: 0.7524, LR: 6.99e-06\n[Rank 1] Step 1560/2500, Loss: 0.7626, LR: 6.99e-06\n[Rank 0] Step 1600/2500, Loss: 0.7511, LR: 6.69e-06[Rank 1] Step 1600/2500, Loss: 0.7608, LR: 6.69e-06\n\n[Rank 0] Step 1640/2500, Loss: 0.7516, LR: 6.40e-06\n[Rank 1] Step 1640/2500, Loss: 0.7587, LR: 6.40e-06\n[Rank 1] Step 1680/2500, Loss: 0.7570, LR: 6.10e-06[Rank 0] Step 1680/2500, Loss: 0.7494, LR: 6.10e-06\n\n[Rank 0] Step 1720/2500, Loss: 0.7492, LR: 5.81e-06\n[Rank 1] Step 1720/2500, Loss: 0.7554, LR: 5.81e-06\n[Rank 1] Step 1760/2500, Loss: 0.7543, LR: 5.51e-06[Rank 0] Step 1760/2500, Loss: 0.7479, LR: 5.51e-06\n\n[Rank 0] Step 1800/2500, Loss: 0.7473, LR: 5.21e-06\n[Rank 1] Step 1800/2500, Loss: 0.7546, LR: 5.21e-06\n[Rank 0] Step 1840/2500, Loss: 0.7464, LR: 4.92e-06[Rank 1] Step 1840/2500, Loss: 0.7551, LR: 4.92e-06\n\n[Rank 1] Step 1880/2500, Loss: 0.7531, LR: 4.62e-06[Rank 0] Step 1880/2500, Loss: 0.7443, LR: 4.62e-06\n\n[Rank 0] Step 1920/2500, Loss: 0.7432, LR: 4.32e-06[Rank 1] Step 1920/2500, Loss: 0.7515, LR: 4.32e-06\n\n[Rank 1] Step 1960/2500, Loss: 0.7493, LR: 4.03e-06[Rank 0] Step 1960/2500, Loss: 0.7419, LR: 4.03e-06\n\n[Rank 1] Step 2000/2500, Loss: 0.7478, LR: 3.73e-06[Rank 0] Step 2000/2500, Loss: 0.7408, LR: 3.73e-06\n\n[Rank 1] Step 2040/2500, Loss: 0.7457, LR: 3.44e-06[Rank 0] Step 2040/2500, Loss: 0.7393, LR: 3.44e-06\n\n[Rank 0] Step 2080/2500, Loss: 0.7378, LR: 3.14e-06[Rank 1] Step 2080/2500, Loss: 0.7460, LR: 3.14e-06\n\n[Rank 0] Step 2120/2500, Loss: 0.7369, LR: 2.84e-06\n[Rank 1] Step 2120/2500, Loss: 0.7452, LR: 2.84e-06\n[Rank 0] Step 2160/2500, Loss: 0.7358, LR: 2.55e-06[Rank 1] Step 2160/2500, Loss: 0.7441, LR: 2.55e-06\n\n[Rank 0] Step 2200/2500, Loss: 0.7359, LR: 2.25e-06\n[Rank 1] Step 2200/2500, Loss: 0.7420, LR: 2.25e-06\n[Rank 0] Step 2240/2500, Loss: 0.7343, LR: 1.95e-06[Rank 1] Step 2240/2500, Loss: 0.7412, LR: 1.95e-06\n\n[Rank 1] Step 2280/2500, Loss: 0.7401, LR: 1.66e-06[Rank 0] Step 2280/2500, Loss: 0.7341, LR: 1.66e-06\n\n[Rank 1] Step 2320/2500, Loss: 0.7408, LR: 1.36e-06[Rank 0] Step 2320/2500, Loss: 0.7342, LR: 1.36e-06\n\n[Rank 0] Step 2360/2500, Loss: 0.7334, LR: 1.07e-06[Rank 1] Step 2360/2500, Loss: 0.7397, LR: 1.07e-06\n\n[Rank 0] Step 2400/2500, Loss: 0.7320, LR: 7.70e-07[Rank 1] Step 2400/2500, Loss: 0.7380, LR: 7.70e-07\n\n[Rank 0] Step 2440/2500, Loss: 0.7324, LR: 4.74e-07[Rank 1] Step 2440/2500, Loss: 0.7365, LR: 4.74e-07\n\n[Rank 1] Step 2480/2500, Loss: 0.7355, LR: 1.78e-07[Rank 0] Step 2480/2500, Loss: 0.7316, LR: 1.78e-07\n\nTraining Time: 41 mins 28 seconds\nTraining Loss: 0.7328\nValidating...\n[Rank 0] Step 40/2500, Loss: 1.4545[Rank 1] Step 40/2500, Loss: 1.6404\n\n[Rank 0] Step 80/2500, Loss: 1.3983[Rank 1] Step 80/2500, Loss: 1.5620\n\n[Rank 0] Step 120/2500, Loss: 1.4388[Rank 1] Step 120/2500, Loss: 1.5976\n\n[Rank 0] Step 160/2500, Loss: 1.4562[Rank 1] Step 160/2500, Loss: 1.5938\n\n[Rank 0] Step 200/2500, Loss: 1.4625[Rank 1] Step 200/2500, Loss: 1.5639\n\n[Rank 0] Step 240/2500, Loss: 1.4533[Rank 1] Step 240/2500, Loss: 1.5497\n\n[Rank 1] Step 280/2500, Loss: 1.5436[Rank 0] Step 280/2500, Loss: 1.4626\n\n[Rank 0] Step 320/2500, Loss: 1.4396[Rank 1] Step 320/2500, Loss: 1.5076\n\n[Rank 1] Step 360/2500, Loss: 1.4936[Rank 0] Step 360/2500, Loss: 1.4160\n\n[Rank 0] Step 400/2500, Loss: 1.4339[Rank 1] Step 400/2500, Loss: 1.4765\n\n[Rank 0] Step 440/2500, Loss: 1.4324[Rank 1] Step 440/2500, Loss: 1.4637\n\n[Rank 1] Step 480/2500, Loss: 1.4643[Rank 0] Step 480/2500, Loss: 1.4180\n\n[Rank 1] Step 520/2500, Loss: 1.4789[Rank 0] Step 520/2500, Loss: 1.4249\n\n[Rank 1] Step 560/2500, Loss: 1.4715[Rank 0] Step 560/2500, Loss: 1.4307\n\n[Rank 1] Step 600/2500, Loss: 1.4748[Rank 0] Step 600/2500, Loss: 1.4319\n\n[Rank 1] Step 640/2500, Loss: 1.4877[Rank 0] Step 640/2500, Loss: 1.4309\n\n[Rank 0] Step 680/2500, Loss: 1.4429[Rank 1] Step 680/2500, Loss: 1.4928\n\n[Rank 1] Step 720/2500, Loss: 1.4993[Rank 0] Step 720/2500, Loss: 1.4512\n\n[Rank 0] Step 760/2500, Loss: 1.4470[Rank 1] Step 760/2500, Loss: 1.5050\n\n[Rank 1] Step 800/2500, Loss: 1.5038[Rank 0] Step 800/2500, Loss: 1.4518\n\n[Rank 0] Step 840/2500, Loss: 1.4535[Rank 1] Step 840/2500, Loss: 1.5007\n\n[Rank 0] Step 880/2500, Loss: 1.4411[Rank 1] Step 880/2500, Loss: 1.4877\n\n[Rank 0] Step 920/2500, Loss: 1.4432[Rank 1] Step 920/2500, Loss: 1.4890\n\n[Rank 1] Step 960/2500, Loss: 1.4812[Rank 0] Step 960/2500, Loss: 1.4419\n\n[Rank 1] Step 1000/2500, Loss: 1.4807[Rank 0] Step 1000/2500, Loss: 1.4491\n\n[Rank 1] Step 1040/2500, Loss: 1.4794[Rank 0] Step 1040/2500, Loss: 1.4443\n\n[Rank 0] Step 1080/2500, Loss: 1.4412\n[Rank 1] Step 1080/2500, Loss: 1.4786\n[Rank 0] Step 1120/2500, Loss: 1.4420[Rank 1] Step 1120/2500, Loss: 1.4774\n\n[Rank 1] Step 1160/2500, Loss: 1.4818[Rank 0] Step 1160/2500, Loss: 1.4378\n\n[Rank 1] Step 1200/2500, Loss: 1.4861[Rank 0] Step 1200/2500, Loss: 1.4381\n\n[Rank 0] Step 1240/2500, Loss: 1.4371[Rank 1] Step 1240/2500, Loss: 1.4800\n\n[Rank 1] Step 1280/2500, Loss: 1.4825[Rank 0] Step 1280/2500, Loss: 1.4349\n\n[Rank 1] Step 1320/2500, Loss: 1.4812[Rank 0] Step 1320/2500, Loss: 1.4353\n\n[Rank 1] Step 1360/2500, Loss: 1.4787[Rank 0] Step 1360/2500, Loss: 1.4346\n\n[Rank 1] Step 1400/2500, Loss: 1.4829[Rank 0] Step 1400/2500, Loss: 1.4372\n\n[Rank 0] Step 1440/2500, Loss: 1.4322[Rank 1] Step 1440/2500, Loss: 1.4825\n\n[Rank 0] Step 1480/2500, Loss: 1.4343\n[Rank 1] Step 1480/2500, Loss: 1.4841\n[Rank 0] Step 1520/2500, Loss: 1.4348[Rank 1] Step 1520/2500, Loss: 1.4865\n\n[Rank 1] Step 1560/2500, Loss: 1.4866[Rank 0] Step 1560/2500, Loss: 1.4377\n\n[Rank 0] Step 1600/2500, Loss: 1.4412[Rank 1] Step 1600/2500, Loss: 1.4860\n\n[Rank 1] Step 1640/2500, Loss: 1.4854[Rank 0] Step 1640/2500, Loss: 1.4418\n\n[Rank 1] Step 1680/2500, Loss: 1.4817[Rank 0] Step 1680/2500, Loss: 1.4425\n\n[Rank 1] Step 1720/2500, Loss: 1.4805[Rank 0] Step 1720/2500, Loss: 1.4455\n\n[Rank 1] Step 1760/2500, Loss: 1.4836[Rank 0] Step 1760/2500, Loss: 1.4471\n\n[Rank 1] Step 1800/2500, Loss: 1.4855[Rank 0] Step 1800/2500, Loss: 1.4448\n\n[Rank 1] Step 1840/2500, Loss: 1.4822[Rank 0] Step 1840/2500, Loss: 1.4429\n\n[Rank 0] Step 1880/2500, Loss: 1.4406[Rank 1] Step 1880/2500, Loss: 1.4792\n\n[Rank 1] Step 1920/2500, Loss: 1.4803[Rank 0] Step 1920/2500, Loss: 1.4397\n\n[Rank 0] Step 1960/2500, Loss: 1.4429[Rank 1] Step 1960/2500, Loss: 1.4796\n\n[Rank 0] Step 2000/2500, Loss: 1.4412\n[Rank 1] Step 2000/2500, Loss: 1.4783\n[Rank 0] Step 2040/2500, Loss: 1.4407[Rank 1] Step 2040/2500, Loss: 1.4757\n\n[Rank 0] Step 2080/2500, Loss: 1.4436[Rank 1] Step 2080/2500, Loss: 1.4748\n\n[Rank 0] Step 2120/2500, Loss: 1.4431[Rank 1] Step 2120/2500, Loss: 1.4730\n\n[Rank 0] Step 2160/2500, Loss: 1.4390[Rank 1] Step 2160/2500, Loss: 1.4723\n\n[Rank 1] Step 2200/2500, Loss: 1.4727[Rank 0] Step 2200/2500, Loss: 1.4429\n\n[Rank 0] Step 2240/2500, Loss: 1.4416[Rank 1] Step 2240/2500, Loss: 1.4716\n\n[Rank 1] Step 2280/2500, Loss: 1.4749[Rank 0] Step 2280/2500, Loss: 1.4387\n\n[Rank 0] Step 2320/2500, Loss: 1.4400[Rank 1] Step 2320/2500, Loss: 1.4776\n\n[Rank 1] Step 2360/2500, Loss: 1.4789[Rank 0] Step 2360/2500, Loss: 1.4407\n\n[Rank 0] Step 2400/2500, Loss: 1.4411[Rank 1] Step 2400/2500, Loss: 1.4833\n\n[Rank 0] Step 2440/2500, Loss: 1.4386[Rank 1] Step 2440/2500, Loss: 1.4804\n\n[Rank 1] Step 2480/2500, Loss: 1.4782[Rank 0] Step 2480/2500, Loss: 1.4404\n\nValidation Time: 10 mins 6 seconds\nValidation Loss: 1.4422\nPerplexity: 4.23\n\n-------------------------\nRegular save mode\n-------------------------\n[Rank 1] Done writing model state dict\n[Rank 0] Done writing model state dict\nSaving model...\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mFSDP\u001b[0m at: \u001b[34mhttps://wandb.ai/tungnguyen19995969-hanoi-university-of-science-and-techn/PARADIS-Qwen3_0.6B/runs/br6n0x73\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250621_130843-br6n0x73/logs\u001b[0m\nModel at epoch 2 saved to ./qwen-vietnamese-wiki-finetuned/last-model-ckpt.pt\nlast-model-ckpt.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.01G/3.01G [00:59<00:00, 50.7MB/s]\nAlso saved to Huggingface repo Quoc59/PARADIS-Qwen3_0.6B-10kWikiVi-FSDP\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ‚ñÅ‚ñÖ‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m:  learning_rate ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:     perplexity ‚ñÑ‚ñÅ‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:     train_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m: train_time (m) ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:     valid_loss ‚ñÑ‚ñÅ‚ñà\n\u001b[34m\u001b[1mwandb\u001b[0m: valid_time (m) ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 3\n\u001b[34m\u001b[1mwandb\u001b[0m:  learning_rate 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m:     perplexity 4.22993\n\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.7316\n\u001b[34m\u001b[1mwandb\u001b[0m:     train_step 7480\n\u001b[34m\u001b[1mwandb\u001b[0m: train_time (m) 41\n\u001b[34m\u001b[1mwandb\u001b[0m:     valid_loss 1.44219\n\u001b[34m\u001b[1mwandb\u001b[0m: valid_time (m) 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mFSDP\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tungnguyen19995969-hanoi-university-of-science-and-techn/PARADIS-Qwen3_0.6B/runs/br6n0x73\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tungnguyen19995969-hanoi-university-of-science-and-techn/PARADIS-Qwen3_0.6B\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250621_130840-br6n0x73/logs\u001b[0m\n","output_type":"stream"}],"execution_count":5}]}