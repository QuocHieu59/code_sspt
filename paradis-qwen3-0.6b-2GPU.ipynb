{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f261ae",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-20T20:56:05.826410Z",
     "iopub.status.busy": "2025-06-20T20:56:05.826200Z",
     "iopub.status.idle": "2025-06-20T20:56:07.033003Z",
     "shell.execute_reply": "2025-06-20T20:56:07.032263Z"
    },
    "papermill": {
     "duration": 1.210483,
     "end_time": "2025-06-20T20:56:07.034293",
     "exception": false,
     "start_time": "2025-06-20T20:56:05.823810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PARADIS-final_project'...\r\n",
      "remote: Enumerating objects: 189, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (36/36), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\r\n",
      "remote: Total 189 (delta 11), reused 35 (delta 11), pack-reused 153 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (189/189), 37.05 KiB | 1.48 MiB/s, done.\r\n",
      "Resolving deltas: 100% (61/61), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tqhb2502/PARADIS-final_project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7eaacf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:56:07.038868Z",
     "iopub.status.busy": "2025-06-20T20:56:07.038654Z",
     "iopub.status.idle": "2025-06-20T20:56:07.044840Z",
     "shell.execute_reply": "2025-06-20T20:56:07.044280Z"
    },
    "papermill": {
     "duration": 0.009653,
     "end_time": "2025-06-20T20:56:07.045845",
     "exception": false,
     "start_time": "2025-06-20T20:56:07.036192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/PARADIS-final_project\n"
     ]
    }
   ],
   "source": [
    "%cd PARADIS-final_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51992bd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:56:07.049462Z",
     "iopub.status.busy": "2025-06-20T20:56:07.049296Z",
     "iopub.status.idle": "2025-06-20T20:56:07.365147Z",
     "shell.execute_reply": "2025-06-20T20:56:07.364255Z"
    },
    "papermill": {
     "duration": 0.319127,
     "end_time": "2025-06-20T20:56:07.366465",
     "exception": false,
     "start_time": "2025-06-20T20:56:07.047338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f6f5bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T20:56:07.370681Z",
     "iopub.status.busy": "2025-06-20T20:56:07.370429Z",
     "iopub.status.idle": "2025-06-20T20:58:24.747542Z",
     "shell.execute_reply": "2025-06-20T20:58:24.746615Z"
    },
    "papermill": {
     "duration": 137.38102,
     "end_time": "2025-06-20T20:58:24.749150",
     "exception": false,
     "start_time": "2025-06-20T20:56:07.368130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0620 20:56:16.078000 57 torch/distributed/run.py:792] \r\n",
      "W0620 20:56:16.078000 57 torch/distributed/run.py:792] *****************************************\r\n",
      "W0620 20:56:16.078000 57 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W0620 20:56:16.078000 57 torch/distributed/run.py:792] *****************************************\r\n",
      "2025-06-20 20:56:39.542659: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-06-20 20:56:39.542645: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750453000.006862      60 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750453000.006846      59 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750453000.120188      59 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1750453000.120196      60 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "[Rank 1] Starting with world_size = 2\r\n",
      "[Rank 0] Starting with world_size = 2\r\n",
      "[Rank 0] Process group initialized!\r\n",
      "[Rank 0] Backend: nccl\r\n",
      "[Rank 0] World Size: 2\r\n",
      "[Rank 0] NCCL_DEBUG: NOT SET\r\n",
      "[Rank 1] Process group initialized!\r\n",
      "[Rank 1] Backend: nccl\r\n",
      "[Rank 1] World Size: 2\r\n",
      "[Rank 1] NCCL_DEBUG: NOT SET\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtqhb2502\u001b[0m (\u001b[33mtqhb2502-hanoi-university-of-science-and-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `shared` mode feature is experimental and may change. Please contact support@wandb.com for guidance and to report any issues.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtqhb2502\u001b[0m (\u001b[33mtqhb2502-hanoi-university-of-science-and-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/PARADIS-final_project/wandb/run-20250620_205701-ahjgz8xe\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFSDP\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tqhb2502-hanoi-university-of-science-and-technology/PARADIS-Qwen3_1.7B\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tqhb2502-hanoi-university-of-science-and-technology/PARADIS-Qwen3_1.7B/runs/ahjgz8xe\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `shared` mode feature is experimental and may change. Please contact support@wandb.com for guidance and to report any issues.\r\n",
      "\r\n",
      "==================================================\r\n",
      "General setup has been done!\r\n",
      "==================================================\r\n",
      "\r\n",
      "==================================================\r\n",
      "Model & Tokenizer\r\n",
      "==================================================\r\n",
      "Loading tokenizer and model...\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/PARADIS-final_project/wandb/run-20250620_205704-ahjgz8xe\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFSDP\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tqhb2502-hanoi-university-of-science-and-technology/PARADIS-Qwen3_1.7B\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tqhb2502-hanoi-university-of-science-and-technology/PARADIS-Qwen3_1.7B/runs/ahjgz8xe\u001b[0m\r\n",
      "tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.73k/9.73k [00:00<00:00, 14.7MB/s]\r\n",
      "vocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.78M/2.78M [00:00<00:00, 13.7MB/s]\r\n",
      "merges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.67M/1.67M [00:00<00:00, 24.4MB/s]\r\n",
      "tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11.4M/11.4M [00:00<00:00, 21.1MB/s]\r\n",
      "config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 726/726 [00:00<00:00, 1.22MB/s]\r\n",
      "model.safetensors.index.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25.6k/25.6k [00:00<00:00, 21.7MB/s]\r\n",
      "Fetching 2 files:   0%|                                   | 0/2 [00:00<?, ?it/s]\r\n",
      "model-00002-of-00002.safetensors:   0%|              | 0.00/622M [00:00<?, ?B/s]\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/3.44G [00:00<?, ?B/s]\u001b[A\u001b[A\r\n",
      "model-00002-of-00002.safetensors:   0%|    | 68.9k/622M [00:00<1:23:21, 124kB/s]\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:   0%|   | 95.1k/3.44G [00:00<7:57:09, 120kB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:   1%|    | 20.2M/3.44G [00:01<02:59, 19.1MB/s]\u001b[A\u001b[A\r\n",
      "model-00002-of-00002.safetensors:  11%|‚ñå    | 67.2M/622M [00:01<00:10, 50.8MB/s]\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:   1%|    | 26.4M/3.44G [00:01<03:26, 16.5MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:   3%|‚ñè    | 110M/3.44G [00:01<00:35, 93.7MB/s]\u001b[A\u001b[A\r\n",
      "model-00002-of-00002.safetensors:  22%|‚ñà‚ñé    | 134M/622M [00:02<00:07, 66.1MB/s]\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:   5%|‚ñé     | 178M/3.44G [00:02<00:28, 114MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:   9%|‚ñå     | 293M/3.44G [00:02<00:16, 188MB/s]\u001b[A\u001b[A\r\n",
      "model-00002-of-00002.safetensors:  32%|‚ñà‚ñâ    | 201M/622M [00:02<00:05, 77.4MB/s]\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  10%|‚ñå     | 331M/3.44G [00:03<00:22, 139MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  12%|‚ñã     | 410M/3.44G [00:03<00:15, 195MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  14%|‚ñä     | 471M/3.44G [00:03<00:12, 240MB/s]\u001b[A\u001b[A\r\n",
      "model-00002-of-00002.safetensors:  35%|‚ñà‚ñà    | 220M/622M [00:03<00:07, 53.6MB/s]\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  15%|‚ñâ     | 515M/3.44G [00:04<00:28, 102MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  16%|‚ñâ     | 553M/3.44G [00:04<00:25, 115MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  17%|‚ñà     | 580M/3.44G [00:05<00:26, 109MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  18%|‚ñà     | 606M/3.44G [00:05<00:28, 100MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  19%|‚ñà‚ñè    | 651M/3.44G [00:05<00:22, 126MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  20%|‚ñà‚ñè    | 703M/3.44G [00:05<00:17, 160MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  22%|‚ñà‚ñé    | 759M/3.44G [00:06<00:14, 188MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  23%|‚ñà‚ñç    | 795M/3.44G [00:06<00:14, 185MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  24%|‚ñà‚ñç    | 818M/3.44G [00:06<00:14, 177MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  24%|‚ñà‚ñç    | 841M/3.44G [00:06<00:15, 173MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  25%|‚ñà‚ñå    | 869M/3.44G [00:07<00:20, 127MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  26%|‚ñà‚ñå    | 906M/3.44G [00:07<00:16, 151MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  28%|‚ñà‚ñã    | 980M/3.44G [00:07<00:10, 241MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  30%|‚ñà‚ñç   | 1.03G/3.44G [00:07<00:08, 281MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  31%|‚ñà‚ñå   | 1.07G/3.44G [00:07<00:09, 245MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  33%|‚ñà‚ñã   | 1.14G/3.44G [00:08<00:11, 208MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  35%|‚ñà‚ñä   | 1.22G/3.44G [00:08<00:08, 274MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  38%|‚ñà‚ñâ   | 1.31G/3.44G [00:08<00:09, 221MB/s]\u001b[A\u001b[A\r\n",
      "model-00002-of-00002.safetensors:  46%|‚ñà‚ñà‚ñä   | 287M/622M [00:08<00:14, 23.4MB/s]\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  39%|‚ñà‚ñâ   | 1.36G/3.44G [00:08<00:09, 213MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  42%|‚ñà‚ñà   | 1.43G/3.44G [00:09<00:08, 233MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  44%|‚ñà‚ñà‚ñè  | 1.50G/3.44G [00:09<00:06, 284MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  46%|‚ñà‚ñà‚ñé  | 1.57G/3.44G [00:09<00:05, 328MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  47%|‚ñà‚ñà‚ñé  | 1.63G/3.44G [00:09<00:05, 360MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  49%|‚ñà‚ñà‚ñç  | 1.69G/3.44G [00:10<00:07, 245MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  50%|‚ñà‚ñà‚ñå  | 1.73G/3.44G [00:10<00:07, 226MB/s]\u001b[A\u001b[A\r\n",
      "model-00002-of-00002.safetensors:  57%|‚ñà‚ñà‚ñà‚ñç  | 354M/622M [00:10<00:09, 28.0MB/s]\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  52%|‚ñà‚ñà‚ñå  | 1.78G/3.44G [00:10<00:07, 232MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  53%|‚ñà‚ñà‚ñã  | 1.84G/3.44G [00:10<00:07, 219MB/s]\u001b[A\u001b[A\r\n",
      "model-00002-of-00002.safetensors:  68%|‚ñà‚ñà‚ñà‚ñà  | 421M/622M [00:10<00:05, 39.0MB/s]\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  55%|‚ñà‚ñà‚ñã  | 1.88G/3.44G [00:10<00:06, 227MB/s]\u001b[A\u001b[A\r\n",
      "model-00002-of-00002.safetensors:  78%|‚ñà‚ñà‚ñà‚ñà‚ñã | 488M/622M [00:11<00:02, 52.6MB/s]\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  57%|‚ñà‚ñà‚ñä  | 1.97G/3.44G [00:11<00:07, 193MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  59%|‚ñà‚ñà‚ñâ  | 2.03G/3.44G [00:11<00:08, 173MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  62%|‚ñà‚ñà‚ñà  | 2.13G/3.44G [00:12<00:05, 241MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  63%|‚ñà‚ñà‚ñà‚ñè | 2.18G/3.44G [00:12<00:05, 227MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  65%|‚ñà‚ñà‚ñà‚ñè | 2.23G/3.44G [00:12<00:05, 238MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  67%|‚ñà‚ñà‚ñà‚ñé | 2.32G/3.44G [00:13<00:05, 206MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  69%|‚ñà‚ñà‚ñà‚ñç | 2.38G/3.44G [00:13<00:05, 198MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  72%|‚ñà‚ñà‚ñà‚ñå | 2.48G/3.44G [00:13<00:03, 261MB/s]\u001b[A\u001b[A\r\n",
      "model-00002-of-00002.safetensors:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 555M/622M [00:14<00:01, 36.6MB/s]\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  74%|‚ñà‚ñà‚ñà‚ñã | 2.54G/3.44G [00:14<00:05, 168MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  77%|‚ñà‚ñà‚ñà‚ñä | 2.66G/3.44G [00:14<00:03, 241MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  79%|‚ñà‚ñà‚ñà‚ñâ | 2.70G/3.44G [00:15<00:06, 118MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  80%|‚ñà‚ñà‚ñà‚ñâ | 2.75G/3.44G [00:16<00:06, 100MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  82%|‚ñà‚ñà‚ñà‚ñé| 2.81G/3.44G [00:17<00:08, 77.8MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  84%|‚ñà‚ñà‚ñà‚ñé| 2.88G/3.44G [00:18<00:07, 73.4MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  86%|‚ñà‚ñà‚ñà‚ñç| 2.95G/3.44G [00:19<00:05, 86.8MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  89%|‚ñà‚ñà‚ñà‚ñà‚ñç| 3.07G/3.44G [00:19<00:02, 130MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñã| 3.18G/3.44G [00:20<00:01, 162MB/s]\u001b[A\u001b[A\r\n",
      "model-00002-of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 622M/622M [00:20<00:00, 30.9MB/s]\r\n",
      "\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñã| 3.25G/3.44G [00:20<00:01, 164MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors:  98%|‚ñà‚ñà‚ñà‚ñà‚ñâ| 3.38G/3.44G [00:20<00:00, 243MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 3.44G/3.44G [00:20<00:00, 165MB/s]\r\n",
      "Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:21<00:00, 10.54s/it]\r\n",
      "Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:21<00:00, 10.56s/it]\r\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.87s/it]\r\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.93s/it]\r\n",
      "generation_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 239/239 [00:00<00:00, 623kB/s]\r\n",
      "Model loaded. Parameters: 1,720,574,976\r\n",
      "\r\n",
      "==================================================\r\n",
      "Dataset\r\n",
      "==================================================\r\n",
      "README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 632/632 [00:00<00:00, 1.30MB/s]\r\n",
      "(‚Ä¶)-00000-of-00003-6218d2963e302058.parquet: 100%|‚ñà| 245M/245M [00:01<00:00, 216\r\n",
      "(‚Ä¶)-00001-of-00003-12e6c4fadbec91d4.parquet: 100%|‚ñà| 55.2M/55.2M [00:00<00:00, 1\r\n",
      "(‚Ä¶)-00002-of-00003-175fcfe1c45b0b85.parquet: 100%|‚ñà| 270M/270M [00:01<00:00, 235\r\n",
      "Generating train split: 100%|‚ñà| 1284930/1284930 [00:04<00:00, 301955.19 examples\r\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1284930/1284930 [00:09<00:00, 140981.64 examples/s]\r\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1284930/1284930 [00:09<00:00, 137294.45 examples/s]\r\n",
      "Total: 1263196 samples\r\n",
      "Train samples: 10000\r\n",
      "Valid samples: 10000\r\n",
      "Train batches: 2500\r\n",
      "Valid batches: 2500\r\n",
      "\r\n",
      "==================================================\r\n",
      "Optimizer & scheduler\r\n",
      "==================================================\r\n",
      "Total training steps: 1562\r\n",
      "Warmup steps: 156\r\n",
      "\r\n",
      "==================================================\r\n",
      "Epoch 1/5\r\n",
      "==================================================\r\n",
      "Training...\r\n",
      "Error in rank 1: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 1 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 5726 has 14.73 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 374.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n",
      "Error in rank 0: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 5725 has 14.73 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 374.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/PARADIS-final_project/PARADIS-Qwen3-FSDP.py\", line 840, in <module>\r\n",
      "    main()\r\n",
      "  File \"/kaggle/working/PARADIS-final_project/PARADIS-Qwen3-FSDP.py\", line 832, in main\r\n",
      "    fsdp_training(rank, world_size)\r\n",
      "  File \"/kaggle/working/PARADIS-final_project/PARADIS-Qwen3-FSDP.py\", line 810, in fsdp_training\r\n",
      "    cleanup_fsdp()\r\n",
      "  File \"/kaggle/working/PARADIS-final_project/PARADIS-Qwen3-FSDP.py\", line 547, in cleanup_fsdp\r\n",
      "    dist.destroy_process_group()\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 2146, in destroy_process_group\r\n",
      "    _shutdown_backend(pg_to_shutdown)\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1815, in _shutdown_backend\r\n",
      "    backend._shutdown()\r\n",
      "torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\r\n",
      "ncclUnhandledCudaError: Call to CUDA function failed.\r\n",
      "Last error:\r\n",
      "Cuda failure 'out of memory'\r\n",
      "[rank1]: Traceback (most recent call last):\r\n",
      "[rank1]:   File \"/kaggle/working/PARADIS-final_project/PARADIS-Qwen3-FSDP.py\", line 840, in <module>\r\n",
      "[rank1]:     main()\r\n",
      "[rank1]:   File \"/kaggle/working/PARADIS-final_project/PARADIS-Qwen3-FSDP.py\", line 832, in main\r\n",
      "[rank1]:     fsdp_training(rank, world_size)\r\n",
      "[rank1]:   File \"/kaggle/working/PARADIS-final_project/PARADIS-Qwen3-FSDP.py\", line 810, in fsdp_training\r\n",
      "[rank1]:     cleanup_fsdp()\r\n",
      "[rank1]:   File \"/kaggle/working/PARADIS-final_project/PARADIS-Qwen3-FSDP.py\", line 547, in cleanup_fsdp\r\n",
      "[rank1]:     dist.destroy_process_group()\r\n",
      "[rank1]:   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 2146, in destroy_process_group\r\n",
      "[rank1]:     _shutdown_backend(pg_to_shutdown)\r\n",
      "[rank1]:   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1815, in _shutdown_backend\r\n",
      "[rank1]:     backend._shutdown()\r\n",
      "[rank1]: torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\r\n",
      "[rank1]: ncclUnhandledCudaError: Call to CUDA function failed.\r\n",
      "[rank1]: Last error:\r\n",
      "[rank1]: Cuda failure 'out of memory'\r\n",
      "\u001b[1;34mwandb\u001b[0m: \r\n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mFSDP\u001b[0m at: \u001b[34mhttps://wandb.ai/tqhb2502-hanoi-university-of-science-and-technology/PARADIS-Qwen3_1.7B/runs/ahjgz8xe\u001b[0m\r\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250620_205704-ahjgz8xe/logs\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mFSDP\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tqhb2502-hanoi-university-of-science-and-technology/PARADIS-Qwen3_1.7B/runs/ahjgz8xe\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tqhb2502-hanoi-university-of-science-and-technology/PARADIS-Qwen3_1.7B\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250620_205701-ahjgz8xe/logs\u001b[0m\r\n",
      "E0620 20:58:24.295000 57 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 60) of binary: /usr/bin/python3\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/bin/torchrun\", line 10, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "             ^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\r\n",
      "    return f(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 918, in main\r\n",
      "    run(args)\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 909, in run\r\n",
      "    elastic_launch(\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\r\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\r\n",
      "    raise ChildFailedError(\r\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n",
      "============================================================\r\n",
      "PARADIS-Qwen3-FSDP.py FAILED\r\n",
      "------------------------------------------------------------\r\n",
      "Failures:\r\n",
      "  <NO_OTHER_FAILURES>\r\n",
      "------------------------------------------------------------\r\n",
      "Root Cause (first observed failure):\r\n",
      "[0]:\r\n",
      "  time      : 2025-06-20_20:58:24\r\n",
      "  host      : 6a6d9fb76688\r\n",
      "  rank      : 1 (local_rank: 1)\r\n",
      "  exitcode  : 1 (pid: 60)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "============================================================\r\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 PARADIS-Qwen3-FSDP.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 145.504653,
   "end_time": "2025-06-20T20:58:25.080336",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-20T20:55:59.575683",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
